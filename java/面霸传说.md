## 题目

### java

**1.String、StringBuffer 和 StringBuilder 的区别?**

​		String 不可变，StringBuffer 和 StringBuilder 可变

​		StringBuffer和StringBulider都是用来拼接字符串的，StringBulider不是线程安全的，StringBuffer是线程安全的（StringBuffer对操作方法都加上了synchronize关键字）。



**2.String 的内部实现?String 对象主要存储在哪块区域?**

​		JDK8 中内部实现实际上是一个char[] 数组。value 数组被声明为 final，这意味着 value 数组初始化之后就不能再引用其它数组。并且 String 内部没有改变 value 数组的方法，因此可以保证 String 不可变。

```java
//数组定义为常量，不可修改 
private final char value[];
```

​		在 Java 9 之后，String 类的实现改用 byte 数组存储字符串，同时使用 `coder` 来标识使用了哪种编码

```java
public final class String
    implements java.io.Serializable, Comparable<String>, CharSequence {
    /** The value is used for character storage. */
    private final byte[] value;

    /** The identifier of the encoding used to encode the bytes in {@code value}. */
    private final byte coder;
}
```

​		在JDK1.7之前，StringPool被放在运行时常量池，所以属于永久代，在1.7之后被移到了堆中，因为永久代空间有限，容易引发OOM。



**3.String为什么要声明成final类？（不可变的好处？）**

* 可以缓存 hash 值

因为 String 的 hash 值经常被使用，例如 String 用做 HashMap 的 key。不可变的特性可以使得 hash 值也不可变，因此只需要进行一次计算。

* String Pool 的需要

如果一个 String 对象已经被创建过了，那么就会从 String Pool 中取得引用。只有 String 是不可变的，才可能使用 String Pool。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/image-20191210004132894.png"/> </div><br>
* 安全性

String 经常作为参数，String 不可变性可以保证参数不可变。例如在作为网络连接参数的情况下如果 String 是可变的，那么在网络连接过程中，String 被改变，改变 String 的那一方以为现在连接的是其它主机，而实际情况却不一定是。

* 线程安全  

String 不可变性天生具备线程安全，可以在多个线程中安全地使用。



**4.HashMap 和 HashSet 的区别？HashSet是怎么实现的？**

HashMap最顶层实现的是Map接口，HashSet最顶层实现的是Collection接口；Map接口一般存放的是键值对，而Collection接口存放的是单一元素；HashMap是用键来计算hash值判断元素是否重复，HashSet使用值对象来计算hash值

HashSet的底层实际就是使用的HashMap,将值对象当作HashMap的key（不可重复），值对象为同一对象。

```java
public class HashSet<E>
    extends AbstractSet<E>
    implements Set<E>, Cloneable, java.io.Serializable
{

    private transient HashMap<E,Object> map;

    // Dummy value to associate with an Object in the backing Map
    private static final Object PRESENT = new Object();
    
    public boolean add(E e) {
        return map.put(e, PRESENT)==null;
    }
```



**5.HashMap、HashTable 和 ConcurrentHashMap 的区别**

HashMap是线程不安全的，HashTable(Synchronized)和ConcurrentHashMap(1.8之前使用分段锁，1.8并发控制使用Synchronized和CAS来操作) 是线程安全的

HashMap和ConcurrentHashMap是Map接口的实现，HashTable是基于Dictionary抽象类

HashMap允许空（null）键值（key）而Hashtable、ConcurrentHashMap 不允许





**6.Map的接口实现**

HashMap,TreeMap,LinkedHashMap,hashtable

Map主要用于存储健值对，根据键得到值，因此不允许键重复(重复了覆盖了),但允许值重复。
Hashmap 是一个最常用的Map,它根据键的HashCode值存储数据,根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的。 HashMap最多只允许一条记录的键为Null;允许多条记录的值为 Null;HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。如果需要同步，可以用 Collections的synchronizedMap方法使HashMap具有同步的能力，或者使用ConcurrentHashMap。

Hashtable与 HashMap类似,它继承自Dictionary类，不同的是:它不允许记录的键或者值为空;它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了 Hashtable在写入时会比较慢。

LinkedHashMap 是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的.也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比HashMap慢，不过有种情况例外，当HashMap容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap慢，因为LinkedHashMap的遍历速度只和实际数据有关，和容量无关，而HashMap的遍历速度和他的容量有关。

TreeMap实现SortMap接口，能够把它保存的记录根据键排序,默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator 遍历TreeMap时，得到的记录是排过序的。

一般情况下，我们用的最多的是HashMap,在Map 中插入、删除和定位元素，HashMap 是最好的选择。但如果您要按自然顺序或自定义顺序遍历键，那么TreeMap会更好。如果需要输出的顺序和输入的相同,那么用LinkedHashMap 可以实现,它还可以按读取顺序来排列.

HashMap是一个最常用的Map，它根据键的hashCode值存储数据，根据键可以直接获取它的值，具有很快的访问速度。HashMap最多只允许一条记录的键为NULL，允许多条记录的值为NULL。

HashMap不支持线程同步，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致性。如果需要同步，可以用Collections的synchronizedMap方法使HashMap具有同步的能力。

Hashtable与HashMap类似，不同的是：它不允许记录的键或者值为空；它支持线程的同步，即任一时刻只有一个线程能写Hashtable，因此也导致了Hashtable在写入时会比较慢。

LinkedHashMap保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的。

在遍历的时候会比HashMap慢TreeMap能够把它保存的记录根据键排序，默认是按升序排序，也可以指定排序的比较器。当用Iterator遍历TreeMap时，得到的记录是排过序的。



**7.LinkedHashMap的实现机制**

LinkedHashMap继承自HashMap，它的多种操作都是建立在HashMap操作的基础上的。同HashMap不同的是，**LinkedHashMap维护了一个Entry的双向链表，保证了插入的Entry中的顺序**。

LinkedHashMap里面的entry继承了HashMap的entry，并且增加了双向指针

```java
static class Entry<K,V> extends HashMap.Node<K,V> {
    Entry<K,V> before, after;
    Entry(int hash, K key, V value, Node<K,V> next) {
        super(hash, key, value, next);
    }
}

	/**
	* 头指针，指向第一个node
	*/
	transient LinkedHashMap.Entry<K,V> head;

    /**
     * 尾指针，指向最后一个node
     */
    transient LinkedHashMap.Entry<K,V> tail;

    /**
     * 一个条件变量，它控制了是否在get操作后需要将新的get的节点重新放到链表的尾部
     * LinkedHashMap可以维持了插入的顺序，但是这个顺序不是不变的，可以被get操作打乱。
     *
     * @serial
     */
    final boolean accessOrder;

```

[超详细LinkedHashMap解析](https://blog.csdn.net/qq_40050586/article/details/105851970)





**8.创建对象的过程**

* 检查类是否被加载
* 为对象分配内存（**类加载完成以后**，虚**拟机就开始为对象分配内存**，此时所需内存的**大小就已经确定**了。只需要在堆上分配所需要的内存即可）
* 为分配的内存空间初始化零值（将对象的内存空间都初**始化为零值**，这样能**保证对象即使没有赋初值，也可以直接使用**）
* 对对象进行其他的设置（**设置的地方都在对象头中**，包括这个**对象所属的类，类的元数据信息，对象的hashcode，GC分代年龄**等信息）
* 执行init方法

创建对象的几种方式：

* 使用new关键字

* Class对象的newInstance()方法

* 构造函数对象的newInstance()方法

* 对象反序列化

* Object对象的clone()方法
* 使用Unsafe类创建对象

在创建对象的过程中，jvm会首先去元空间检查是否有这个类的信息，如果没有则执行类的加载过程，有的话则创建对象。

对象的创建过程中，（**在为这些实例变量分配内存的同时，这些实例变量也会被赋予默认值(零值)。**）初始化顺序：

父类静态变量/父类静态方法块-> 子类静态变量/子类静态代码块->父类成员变量/方法块->父类构造函数->子类成员变量/方法快->子类构造函数

小结：创建一个对象包含下面两个过程：

1、类构造器完成类初始化（分配内存、赋予默认值）

2、类实例化（赋予给定值）

[Java 中创建一个对象的过程？](https://www.cnblogs.com/JonaLin/p/12674281.html)



**9.判断一个对象是否被回收**

判断对象是否被回收一般有两种算法，引用技术法和可达性分析算法，引用计数法无法解决对象循环引用的问题，因此java里面判断对象是否能被回收采用的是可达性分析算法。

可达性分析算法:以GC Roots为起点，可达的都是存活的，不可达的都是需要被回收的。在java中能作为GC Roots的一般是：

- 虚拟机栈中局部变量表中引用的对象
- 本地方法栈中 JNI 中引用的对象
- 方法区中类静态属性引用的对象
- 方法区中的常量引用的对象



**10.新生代和老年代用的垃圾回收策略？**

垃圾回收策略一般有一下几种：标记-清除（标记和清除过程效率都不高，而且会产生内存碎片），标记-整理（需要移动大对象，但是不会产生内存碎片），复制-清除（内存使用率低）。所以一般而言针对不同代采用不同的算法。

- 新生代使用：复制算法
- 老年代使用：标记 - 清除 或者 标记 - 整理 算法



**11.强引用、弱引用、软引用和虚引用的区别**

被强引用关联的对象不会被回收。

被软引用关联的对象只有在内存不够的情况下才会被回收。

被弱引用关联的对象一定会被回收，也就是说它只能存活到下一次垃圾回收发生之前。

虚引用又称为幽灵引用或者幻影引用，一个对象是否有虚引用的存在，不会对其生存时间造成影响，也无法通过虚引用得到一个对象。



**12.final 关键字的作用，final 在多线程并发条件下的作用**

- 对于基本类型，final 使数值不变；
- 对于引用类型，final 使引用不变，也就不能引用其它对象，但是被引用的对象本身是可以修改的。
- 声明方法不能被子类重写。
- 声明类不允许被继承。

final 在多线程并发条件下的作用原理是通过禁止cpu的指令集重排序

1. 在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。
2. 初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。

在Java语言里面（特指JDK 5以后，即Java内存模型被修正之后的Java语言），不可变（Immutable）的对象一定是线程安全的，无论是对象的方法实现还是方法的调用者，都不需要再进行任何线程安全保障措施。

多线程共享的数据是一个基本数据类型，那么只要在定义时使用final关键字修饰它就可以保证它是不可变的。如果共享数据是一个对象，由于Java语言目前暂时还没有提供值类型的支持，那就需要对象自行保证其行为不会对其状态产生任何影响才行

[多线程与高并发(五)final关键字](https://www.cnblogs.com/yuanqinnan/p/11231274.html)



**13.synchronized 作用于不同方法和代码块的区别**

修饰方法，方法标识ACC_PUBLIC代表public修饰，ACC_SYNCHRONIZED指明该方法为同步方法

修饰同步代码块，生成monitor指令。执行monitorenter指令，退出同步代码块，执行monitorexit指令，可以看到有2个monitorexit指令，第一个是正常退出执行的，第二个是当异常发生时执行的

* 修饰实例方法，对当前实例对象this加锁

* 修饰静态方法，对当前类的Class对象加锁

* 修饰代码块，指定加锁对象，对给定对象加锁



**14.volatile和synchronized的区别**

* volatile本质是在告诉jvm当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取； synchronized则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住。
* volatile仅能使用在变量级别；synchronized则可以使用在变量、方法、和类级别的
* volatile仅能实现变量的修改可见性，不能保证原子性；而synchronized则可以保证变量的修改可见性和原子性
* volatile不会造成线程的阻塞；synchronized可能会造成线程的阻塞。
* volatile标记的变量不会被编译器优化；synchronized标记的变量可以被编译器优化
  

volatile的可见性是通过内存屏障和禁止重排序实现的。

volatile会在写操作时，会在写操作后加一条store屏障指令，将本地内存中的共享变量值刷新到主内存

volatile在进行读操作时，会在读操作前加一条load指令，从内存中读取共享变量：



**15.哪些方法可以创建线程池**

Executors.newSingleThreadExecutor()  单个线程

Executors.newFixedThreadPool(5) 创建一个固定大小的线程池

Executors.newCachedThreadPool() 创建一个可伸缩的线程池

以上几种方法不推荐，推荐使用ThreadPoolExecutor，上面几个方法底层也是使用ThreadPoolExecutor，只不过参数配置不同，使用ThreadPoolExecutor可以手动设置参数，更加了解线程池的原理和运行机制

```java
public ThreadPoolExecutor(int corePoolSize, //核心线程池大小
                          int maximumPoolSize, //最大线程池大小
                          long keepAliveTime, //非核心线程资源多长时间没人访问释放
                          TimeUnit unit, //时间单位
                          BlockingQueue<Runnable> workQueue,//阻塞队列
                          ThreadFactory threadFactory, //线程工程，一般用默认的
                          RejectedExecutionHandler handler //拒绝策略，当线程池达到最大并且阻塞队列满的拒绝策略
                         ) 
```



**16.数组和链表的区别**

​	数组	链表

* 逻辑结构	（1）数组在内存中连续； (2)使用数组之前，必须事先固定数组长度，不支持动态改变数组大小；(3) 数组元素增加时，有可能会数组越界；(4) 数组元素减少时，会造成内存浪费；（5）数组增删时需要移动其它元素	；（1）链表采用动态内存分配的方式，在内存中不连续 (2)支持动态增加或者删除元素 (3)需要时可以使用malloc或者new来申请内存，不用时使用free或者delete来释放内存

* 内存结构	数组从栈上分配内存，使用方便，但是自由度小	；链表从堆上分配内存，自由度大，但是要注意内存泄漏
* 访问效率	数组在内存中顺序存储，可通过下标访问，访问效率高	；链表访问效率低，如果想要访问某个元素，需要从头遍历
* 越界问题	数组的大小是固定的，所以存在访问越界的风险；	只要可以申请得到链表空间，链表就无越界风险



**17.深拷贝和浅拷贝**

浅拷贝：拷贝对象和原始对象的引用类型引用同一个对象。

深拷贝：拷贝对象和原始对象的引用类型引用不同对象。

[面试题：深拷贝和浅拷贝(超级详细，有内存图)](https://blog.csdn.net/jiang7701037/article/details/98738487)



**18.jvm中，直接内存是什么**

直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是《Java虚拟机规范》中定义的内存区域（对外内存）

在JDK 1.4中新加入了NIO（New Input/Output）类，引入了一种基于通道（Channel）与缓冲区（Buffer）的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆里面的DirectByteBuffer对象作为这块内存的引用进行操作。

自从 JDK 引入 NIO 后，直接内存的使用也越来越普遍。通过 native 方法可以分配堆外内存，通过 DirectByteBuffer 对象来操作。

可以通过 **-XX:MaxDirectMemorySize** 参数来设置最大可用直接内存，如果启动时未设置则默认为最大堆内存大小，即与 -Xmx 相同。即假如最大堆内存为1G，则默认直接内存也为1G，那么 JVM 最大需要的内存大小为2G多一些。当直接内存达到最大限制时就会触发GC，如果回收失败则会引起OutOfMemoryError。



**19.final，finalize， finally有什么区别**

final是修饰符（关键字），修饰的类不能派生新的子类，将变量声明为final,可以保证他们使用中引用不被改变，将方法声明为final，则不能被重写。

finalize则是一个方法名，定义在最顶层的Object类中，这个方法在gc启动，该对象被回收的时候被调用。

finally作为异常处理的一部分，它只能用在try/catch语句中，并且附带一个语句块，表示这段语句最终一定会被执行（不管有没有抛出异常），经常被用在需要释放资源的情况下。



**20.synchronized和Lock的区别**

1.synchronized是关键字，而Lock是一个接口。
2.synchronized会自动释放锁，而Lock必须手动释放锁。
3.synchronized是不可中断的，而Lock可以中断也可以不中断。
4.通过Lock可以指定线程有没有拿到锁，而synchronized不能。
5.synchronized能锁住方法和代码块，而Lock只能锁住代码块。
6.Lock可以使用读锁提高多线程读效率。
7.synchronized是非公平锁，ReentrantLock可以控制是否是公平锁



**21.悲观锁和乐观锁** 

悲观锁：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（**共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程**）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中`synchronized`和`ReentrantLock`等独占锁就是悲观锁思想的实现。

乐观锁：总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。**乐观锁适用于多读的应用类型，这样可以提高吞吐量**，像数据库提供的类似于**write_condition机制**，其实都是提供的乐观锁。在Java中`java.util.concurrent.atomic`包下面的原子变量类就是使用了乐观锁的一种实现方式**CAS**实现的。乐观锁的ABA问题可以通过版本号解决。



**22.为什么要使用线程池**

线程池是指在初始化一个多线程应用程序过程中创建一个线程集合，然后在需要执行新的任务时重用这些线程而不是新建一个线程。

**使用线程池的好处**

1. 线程池改进了一个应用程序的响应时间。由于线程池中的线程已经准备好且等待被分配任务，应用程序可以直接拿来使用而不用新建一个线程。
2. 线程池节省了CLR 为每个短生存周期任务创建一个完整的线程的开销并可以在任务完成后回收资源。
3. 线程池根据当前在系统中运行的进程来优化线程时间片。
4. 线程池允许我们开启多个任务而不用为每个线程设置属性。
5. 线程池允许我们为正在执行的任务的程序参数传递一个包含状态信息的对象引用。
6. 线程池可以用来解决处理一个特定请求最大线程数量限制问题。



**23.runnable和callable的区别**

runnable和callable都可以用来编写多线程程序，两者的区别在于：

1.实现了runnable接口后无法返回结果信息，实现了callable接口后有返回值。

2.实现了runnable接口异常无法通过throws抛出异常，实现了callable接口后可以直接抛出Exception异常




### 操作系统

**1.线程的生命周期**

线程的生命周期包含5个阶段，包括：新建、就绪、运行、阻塞、销毁。

- 新建：就是刚使用new方法，new出来的线程；
- 就绪：就是调用的线程的start()方法后，这时候线程处于等待CPU分配资源阶段，谁先抢的CPU资源，谁开始执行;
- 运行：当就绪的线程被调度并获得CPU资源时，便进入运行状态，run方法定义了线程的操作和功能;
- 阻塞：在运行状态的时候，可能因为某些原因导致运行状态的线程变成了阻塞状态，比如sleep()、wait()之后线程就处于了阻塞状态，这个时候需要其他机制将处于阻塞状态的线程唤醒，比如调用notify或者notifyAll()方法。唤醒的线程不会立刻执行run方法，它们要再次等待CPU分配资源进入运行状态;
- 销毁：如果线程正常执行完毕后或线程被提前强制性的终止或出现异常导致结束，那么线程就要被销毁，释放资源;



**2.sleep和wait的区别**

sleep是属于Thread类的**静态**方法，而wait是Object类的**实例**方法；

sleep,wait调用后都会暂停当前线程并让出cpu的执行时间，但不同的是sleep不会释放当前持有的对象的锁资源，到时间后会继续执行，而wait会放弃所有锁并需要notify/notifyAll后重新获取到对象锁资源后才能继续执行

它们都可以被interrupted方法中断。

另外值得一提的是Thread.Sleep(0)的作用，就是触发操作系统立刻重新进行一次CPU竞争，竞争的结果也许是当前线程仍然获得CPU控制权，也许会换成别的线程获得CPU控制权



**3.死锁产生的原因**

* 因为系统资源不足。
* 进程运行推进的顺序不合适。
* 资源分配不当等。

产生死锁的四个必要条件：

* 互斥条件：进程要求对所分配的资源进行排它性控制，即在一段时间内某资源仅为一进程所占用。
* 请求和保持条件：当进程因请求资源而阻塞时，对已获得的资源保持不放。
* 不剥夺条件：进程已获得的资源在未使用完之前，不能剥夺，只能在使用完时由自己释放。
* 环路等待条件：在发生死锁时，必然存在一个进程--资源的环形链。

预防死锁

* 资源一次性分配：一次性分配所有资源，这样就不会再有请求了：（破坏请求条件）
* 只要有一个资源得不到分配，也不给这个进程分配其他的资源：（破坏保持条件）
* 可剥夺资源：即当某进程获得了部分资源，但得不到其它资源，则释放已占有的资源（破坏不可剥夺条件）
* 资源有序分配法：系统给每类资源赋予一个编号，每一个进程按编号递增的顺序请求资源，释放则相反（破坏环路等待条件）



> 什么是线程死锁？死锁如何产生？如何避免线程死锁？
>
> **死锁的介绍**：
>
> 线程死锁是指由于两个或者多个线程互相持有对方所需要的资源，导致这些线程处于等待状态，无法前往执行。当线程进入对象的synchronized代码块时，便占有了资源，直到它退出该代码块或者调用wait方法，才释放资源，在此期间，其他线程将不能进入该代码块。当线程互相持有对方所需要的资源时，会互相等待对方释放资源，如果线程都不主动释放所占有的资源，将产生死锁。
>
> **死锁的产生的一些特定条件**：
>
> 1. 互斥条件：进程对于所分配到的资源具有排它性，即一个资源只能被一个进程占用，直到被该进程释放 。
> 2. 请求和保持条件：一个进程因请求被占用资源而发生阻塞时，对已获得的资源保持不放。
> 3. 不剥夺条件：任何一个资源在没被该进程释放之前，任何其他进程都无法对他剥夺占用。
> 4. 循环等待条件：当发生死锁时，所等待的进程必定会形成一个环路（类似于死循环），造成永久阻塞。
>
> **如何避免**：
>
> **1. 加锁顺序**：当多个线程需要相同的一些锁，但是按照不同的顺序加锁，死锁就很容易发生。如果能确保所有的线程都是按照相同的顺序获得锁，那么死锁就不会发生。当然这种方式需要你事先知道所有可能会用到的锁，然而总有些时候是无法预知的。
>
> **2. 加锁时限**：加上一个超时时间，若一个线程没有在给定的时限内成功获得所有需要的锁，则会进行回退并释放所有已经获得的锁，然后等待一段随机的时间再重试。但是如果有非常多的线程同一时间去竞争同一批资源，就算有超时和回退机制，还是可能会导致这些线程重复地尝试但却始终得不到锁。
>
> **3. 死锁检测**：死锁检测即每当一个线程获得了锁，会在线程和锁相关的数据结构中（map、graph等等）将其记下。除此之外，每当有线程请求锁，也需要记录在这个数据结构中。死锁检测是一个更好的死锁预防机制，它主要是针对那些不可能实现按序加锁并且锁超时也不可行的场景。





**4.在 Linux 系统下，有哪些进程调度的方式**

* 先来先服务 first-come first-serverd（FCFS）

非抢占式的调度算法，按照请求的顺序进行调度。

有利于长作业，但不利于短作业，因为短作业必须一直等待前面的长作业执行完毕才能执行，而长作业又需要执行很长时间，造成了短作业等待时间过长。

* 短作业优先 shortest job first（SJF）

非抢占式的调度算法，按估计运行时间最短的顺序进行调度。

长作业有可能会饿死，处于一直等待短作业执行完毕的状态。因为如果一直有短作业到来，那么长作业永远得不到调度。

* 最短剩余时间优先 shortest remaining time next（SRTN）

最短作业优先的抢占式版本，按剩余运行时间的顺序进行调度。 当一个新的作业到达时，其整个运行时间与当前进程的剩余时间作比较。如果新的进程需要的时间更少，则挂起当前进程，运行新的进程。否则新的进程等待。

* 时间片轮转

将所有就绪进程按 FCFS 的原则排成一个队列，每次调度时，把 CPU 时间分配给队首进程，该进程可以执行一个时间片。当时间片用完时，由计时器发出时钟中断，调度程序便停止该进程的执行，并将它送往就绪队列的末尾，同时继续把 CPU 时间分配给队首的进程。

linux进程分为实时进程（有实时需求的进程）和普通进程两种，实时进程的优先级高于普通进程，故优先执行实时进程。

实时进程的调度方式： 静态优先级策略+先进先出策略/轮转策略
系统调度时，首先执行静态优先级策略，会根据用户设定的静态优先级对实时进程进行排序，先执行优先级高的进程直到完毕，再执行优先级低的进程。当有数个实时进程的优先级相同时，有先进先出策略（SCHED_FIFO）和轮转策略（SCHED_RR）供用户选择。若为先进先出策略，系统会根据进程出现在队列上的位置选择执行的顺序；若为轮转策略，系统会为实时进程分配时间片， 当一个时间片消耗完毕，就会执行下一个相同优先级的进程，如此循环往复，直到同优先级的所有进程执行完毕。

普通进程的调度方式： 动态优先级策略
系统调度时，会关注进程近一段时间内的表现（主要是睡眠时间和执行时间），再根据公式计算动态优先级，并严格按照动态优先级的高低顺序执行进程。其中，动态优先级是以静态优先级为基准进行计算的。若进程为交互式进程（如：桌面程序、服务器等），由于睡眠时间较长，对应的动态优先级也应该较高，当输入到来时能很快地被执行；若进程为批处理进程（如：编译程序），会较长时间处于可执行状态，可适当较低动态优先级，让系统先执行比较紧急的进程。



**5.跨进程通信的方式，信号量怎么理解？**

可以理解为一个计数器



**6.操作系统层面，怎么实现异常中断？**





**7.用户态和内核态有什么区别？**

**1）内核态与用户态是操作系统的两种运行级别，当程序运行在3级特权级上时，就可以称之为运行在用户态。**因为这是最低特权级，是普通的用户进程运行的特权级，大部分用户直接面对的程序都是运行在用户态；

**2）当程序运行在0级特权级上时，就可以称之为运行在内核态。**

**3）运行在用户态下的程序不能直接访问操作系统内核数据结构和程序。当我们在系统中执行一个程序时，大部分时间是运行在用户态下的，在其需要操作系统帮助完成某些它没有权力和能力完成的工作时就会切换到内核态。**

**4）这两种状态的主要差别是**：

- 处于用户态执行时，进程所能访问的内存空间和对象受到限制，其所处于占有的处理机是可被抢占的 ；
- 而处于核心态执行中的进程，则能访问所有的内存空间和对象，且所占有的处理机是不允许被抢占的。



三种情况会导致用户态到内核态的切换

* 系统调用

> 这是用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作

* 异常

> 当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。

* 外围设备中断

> 当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，
>
> 如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。











### 计算机网络



![网络模型以及协议.jpg](http://ww1.sinaimg.cn/large/0072fULUgy1gruh2ipt4pj60zs0dvabw02.jpg)



**1.浏览器中输入域名（url）后发生了什么？**

[键入网址后，期间发生了什么？](https://mp.weixin.qq.com/s/I6BLwbIpfGEJnxjDcPXc1A)

* 解析url，通过DNS解析（DNS解析用的UDP协议）域名成ip地址
* 根据该 IP 地址和默认端口 80，建立TCP连接，3次握手
* 发送http请求
* 服务器处理请求
* 返回http响应
* 关闭TCP连接
* 浏览器解析渲染页面



**2.DNS解析流程**

通过主机名，最终得到该主机名对应的IP地址的过程叫做域名解析（或主机名解析）。

DNS协议运行在UDP协议之上，使用端口号53。

1. 客户端首先会发出一个 DNS 请求，问 www.server.com 的 IP 是啥，并发给本地 DNS 服务器（也就是客户端的 TCP/IP 设置中填写的 DNS 服务器地址）。
2. 本地域名服务器收到客户端的请求后，如果缓存里的表格能找到 www.server.com，则它直接返回 IP 地址。如果没有，本地 DNS 会去问它的根域名服务器：“老大， 能告诉我 www.server.com 的 IP 地址吗？” 根域名服务器是最高层次的，它不直接用于域名解析，但能指明一条道路。
3. 根 DNS 收到来自本地 DNS 的请求后，发现后置是 .com，说：“www.server.com 这个域名归 .com 区域管理”，我给你 .com 顶级域名服务器地址给你，你去问问它吧。”
4. 本地 DNS 收到顶级域名服务器的地址后，发起请求问“老二， 你能告诉我 www.server.com  的 IP 地址吗？”
5. 顶级域名服务器说：“我给你负责 www.server.com 区域的权威 DNS 服务器的地址，你去问它应该能问到”。
6. 本地 DNS 于是转向问权威 DNS 服务器：“老三，www.server.com对应的IP是啥呀？” server.com 的权威 DNS 服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。
7. 权威 DNS 服务器查询后将对应的 IP 地址 X.X.X.X 告诉本地 DNS。
8. 本地 DNS 再将 IP 地址返回客户端，客户端和目标建立连接。



DNS 协议的可靠性的设计？



**3.TCP 的拥塞控制机制**

我们知道TCP通过一个定时器（timer）采样了RTT（Round Trip Time，链路的传播时间propagation delay、末端系统的处理时间、路由器缓存中的排队和处理时间queuing delay）并计算RTO（Retransmission TimeOut，即重传超时时间），但是，如果网络上的延时突然增加，那么，TCP对这个事做出的应对只有重传数据，然而重传会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这就导致了恶性循环，最终形成“网络风暴” —— TCP的拥塞控制机制就是用于应对这种情况。
首先需要了解一个概念，为了在发送端调节所要发送的数据量，定义了一个“拥塞窗口”（Congestion Window），在发送数据时，将拥塞窗口的大小与接收端ack的窗口大小做比较，取较小者作为发送数据量的上限。
拥塞控制主要是四个算法：
1.慢启动：意思是刚刚加入网络的连接，一点一点地提速，不要一上来就把路占满。
连接建好的开始先初始化cwnd = 1，表明可以传一个MSS大小的数据。
每当收到一个ACK，cwnd++; 呈线性上升
每当过了一个RTT，cwnd = cwnd*2; 呈指数让升
阈值ssthresh（slow start threshold），是一个上限，当cwnd >= ssthresh时，就会进入“拥塞避免算法”
2.拥塞避免：当拥塞窗口 cwnd 达到一个阈值时，窗口大小不再呈指数上升，而是以线性上升，避免增长过快导致网络拥塞。
每当收到一个ACK，cwnd = cwnd + 1/cwnd
每当过了一个RTT，cwnd = cwnd + 1
拥塞发生：当发生丢包进行数据包重传时，表示网络已经拥塞。分两种情况进行处理：
等到RTO超时，重传数据包
sshthresh = cwnd /2
cwnd 重置为 1
3.进入慢启动过程
在收到3个duplicate ACK时就开启重传，而不用等到RTO超时
sshthresh = cwnd = cwnd /2
进入快速恢复算法——Fast Recovery
4.快速恢复：至少收到了3个Duplicated Acks，说明网络也不那么糟糕，可以快速恢复。
cwnd = sshthresh + 3 * MSS （3的意思是确认有3个数据包被收到了）
重传Duplicated ACKs指定的数据包
如果再收到 duplicated Acks，那么cwnd = cwnd +1
如果收到了新的Ack，那么，cwnd = sshthresh ，然后就进入了拥塞避免的算法了。



[TCP超时重传、滑动窗口、拥塞控制、快重传和快恢复](https://www.cnblogs.com/postw/p/9678454.html)

[TCP慢启动、拥塞避免、快速重传、快速恢复](https://blog.csdn.net/ydyang1126/article/details/72842274)

**4.Http**

超文本传输协议，HTTP是缩写，它的全英文名是HyperText Transfer Protocol。

> **HTTP 是一个在计算机世界里专门在「两点」之间「传输」文字、图片、音频、视频等「超文本」数据的「约定和规范」。**

**HTTP/1.1、HTTP/2、HTTP/3 演变**

HTTP/1.1 相比 HTTP/1.0 性能上的改进：

- 使用 TCP 长连接的方式改善了 HTTP/1.0 短连接造成的性能开销。
- 支持 管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。

但 HTTP/1.1 还是有性能瓶颈：

- 请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 `Body` 的部分；
- 发送冗长的首部。每次互相发送相同的首部造成的浪费较多；
- 服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据，也就是队头阻塞；
- 没有请求优先级控制；
- 请求只能从客户端开始，服务器只能被动响应。

HTTP/2 协议是基于 HTTPS 的，所以 HTTP/2 的安全性也是有保障的。

那 HTTP/2 相比 HTTP/1.1 性能上的改进：

*1. 头部压缩*

HTTP/2 会**压缩头**（Header）如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你**消除重复的分**。

这就是所谓的 `HPACK` 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就**提高速度**了。

*2. 二进制格式*

HTTP/2 不再像 HTTP/1.1 里的纯文本形式的报文，而是全面采用了**二进制格式。**

头信息和数据体都是二进制，并且统称为帧（frame）：**头信息帧和数据帧**。

*3. 数据流*

HTTP/2 的数据包不是按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。

每个请求或回应的所有数据包，称为一个数据流（`Stream`）。

每个数据流都标记着一个独一无二的编号，其中规定客户端发出的数据流编号为奇数， 服务器发出的数据流编号为偶数

客户端还可以**指定数据流的优先级**。优先级高的请求，服务器就先响应该请求。

*4. 多路复用*

HTTP/2 是可以在**一个连接中并发多个请求或回应，而不用按照顺序一一对应**。

移除了 HTTP/1.1 中的串行请求，不需要排队等待，也就不会再出现「队头阻塞」问题，**降低了延迟，大幅度提高了连接的利用率**。

*5. 服务器推送*

HTTP/2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务不再是被动地响应，也可以**主动**向客户端发送消息。

> HTTP/2 有哪些缺陷？HTTP/3 做了哪些优化？

HTTP/2 主要的问题在于：多个 HTTP 请求在复用一个 TCP 连接，下层的 TCP 协议是不知道有多少个 HTTP 请求的。

所以一旦发生了丢包现象，就会触发 TCP 的重传机制，这样在一个 TCP 连接中的**所有的 HTTP 请求都必须等待这个丢了的包被重传回来**。

- HTTP/1.1 中的管道（ pipeline）传输中如果有一个请求阻塞了，那么队列后请求也统统被阻塞住了
- HTTP/2 多请求复用一个TCP连接，一旦发生丢包，就会阻塞住所有的 HTTP 请求。

这都是基于 TCP 传输层的问题，所以 **HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP！**

![http演进.png](http://ww1.sinaimg.cn/large/0072fULUgy1grk3zxvsp7j60mg0anwjb02.jpg)











**5.HTTP 与 HTTPS 有哪些区别？**

1. HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。
2. HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。
3. HTTP 的端口号是 80，HTTPS 的端口号是 443。
4. HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。

[HTTP 与 HTTPS](https://mp.weixin.qq.com/s/amOya0M00LwpL5kCS96Y6w)



**6.HTTPS是怎么保证安全的**

*1. 混合加密*

HTTPS 采用的是**对称加密**和**非对称加密**结合的「混合加密」方式：

- 在通信建立前采用**非对称加密**的方式交换「会话秘钥」，后续就不再使用非对称加密。
- 在通信过程中全部使用**对称加密**的「会话秘钥」的方式加密明文数据。

采用「混合加密」的方式的原因：

- **对称加密**只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。
- **非对称加密**使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。

*2. 摘要算法*

**摘要算法**用来实现**完整性**，能够为数据生成独一无二的「指纹」，用于校验数据的完整性，解决了篡改的风险。

客户端在发送明文之前会通过摘要算法算出明文的「指纹」，发送的时候把「指纹 + 明文」一同
加密成密文后，发送给服务器，服务器解密后，用相同的摘要算法算出发送过来的明文，通过比较客户端携带的「指纹」和当前算出的「指纹」做比较，若「指纹」相同，说明数据是完整的。

*3. 数字证书*

客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。这就存在些问题，如何保证公钥不被篡改和信任度？

所以这里就需要借助第三方权威机构 `CA` （数字证书认证机构），将**服务器公钥放在数字证书**（由数字证书认证机构颁发）中，只要证书是可信的，公钥就是可信的。



HTTPS 的出发点是解决HTTP明文传输时信息被篡改和监听的问题。

- 为了兼顾性能和安全性，使用了非对称加密+对称加密的方案。
- 为了保证公钥传输中不被篡改，又使用了非对称加密的数字签名功能，借助CA机构和系统根证书的机制保证了HTTPS证书的公信力。



[详解HTTPS加解密数字签名](https://mp.weixin.qq.com/s/21JaXwdfSjItj5SgOwhapg)



**7.对称加密和非对称加密的区别**

对称加密是指有一个密钥，用它可以对一段明文加密，加密之后也只能用这个密钥来解密得到明文。

非对称加密有两个密钥，一个是公钥，另一个是私钥。一般来说，公钥用来加密，这时密文只能用私钥才能解开。



**8.常见的加密算法有哪些？**

* AES（Advanced Encryption Standard）：高级加密标准，对称算法，是下一代的加密算法标准，速度快，安全级别高，在21世纪AES 标准的一个实现是 Rijndael 算法；

* DES（Data Encryption Standard）：对称算法，数据加密标准，速度较快，适用于加密大量数据的场合；

* 3DES（Triple DES）：是基于DES的对称算法，对一块数据用三个不同的密钥进行三次加密，强度更高；

* RC2和RC4：对称算法，用变长密钥对大量数据进行加密，比 DES 快；

* IDEA（International Data Encryption Algorithm）国际数据加密算法，使用 128 位密钥提供非常强的安全性；

* RSA：由 RSA 公司发明，是一个支持变长密钥的公共密钥算法，需要加密的文件块的长度也是可变的，非对称算法；

* DSA（Digital Signature Algorithm）：数字签名算法，是一种标准的 DSS（数字签名标准），严格来说不算加密算法；

* MD5：严格来说不算加密算法，只能说是摘要算法。



**9.SYN泛洪攻击**

SYN泛洪攻击利用TCP三次握手协议的缺陷，向目标主机发送大量的伪造源地址的SYN连接请求，使得被攻击方资源耗尽，从而不能够为正常用户提供服务。

在TCP协议中被称为三次握手（Three-way Handshake）的连接过程中，如果一个用户向服务器发送了SYN报文，服务器又发出 SYN+ACK 应答报文后**未收到客户端的 ACK 报文**，这种情况下服务器端会再次发送SYN+ACK给客户端，并等待一段时间后丢弃这个未完成的连接，这段时间的长度称为SYN Timeout，一般来说这个时间是分钟的数量级。

SYN 泛洪攻击所做的就是利用这个SYN Timeout和TCP/IP协议族中的另一个漏洞: 报文传输过程中对报文的源 IP 地址完全信任进行攻击。SYN 泛洪攻击通过发送大量的伪造 TCP 链接报文而造成大量的 TCP 半连接,服务器端将为了维护这样一个庞大的半连接列表而消耗非常多的资源。这样服务器端将忙于处理攻击者伪造的TCP连接请求而无法处理正常连接请求,甚至会导致堆栈的溢出崩溃
造成SYN洪泛攻击最主要的原因是TCP/IP协议的脆弱性。TCP/IP是一个开放的协议平台，它将越来越多的网络连接在一起，它基于的对象是可信用户群，所以缺少一些必要的安全机制，带来很大的安全威胁。例如常见的IP欺骗、TCP连接的建立、ICMP数据包的发送都存在巨大的安全隐患，给SYN洪泛攻击带来可乘之机。

SYN泛洪防御技术：

- **缩短SYN timeout时间**

* **设置SYN cookie**

* **设置SYN可疑队列**
* **使用防火墙**

[TCP SYN洪泛攻击的原理及防御方法](https://blog.csdn.net/jiangzhengdong/article/details/8119223)



**10.tcp和udp的区别**

* 区别一、是否基于连接
  TCP是面向连接的协议，而UDP是无连接的协议。即TCP面向连接;UDP是无连接的，即发送数据之前不需要建立连接。

* 区别二、可靠性 和 有序性 区别
  TCP 提供交付保证（Tcp通过校验和，重传控制，序号标识，滑动窗口、确认应答实现可靠传输），无差错，不丢失，不重复，且按序到达，也保证了消息的有序性。该消息将以从服务器端发出的同样的顺序发送到客户端，尽管这些消息到网络的另一端时可能是无序的。TCP协议将会为你排好序。
  UDP不提供任何有序性或序列性的保证。UDP尽最大努力交付，数据包将以任何可能的顺序到达。
  TCP的逻辑通信信道是全双工的可靠信道，UDP则是不可靠信道

* 区别三、实时性
  UDP具有较好的实时性，工作效率比TCP高，适用于对高速传输和实时性有较高的通信或广播通信。

* 区别四、协议首部大小
  TCP首部开销20字节; UDP的首部开销小，只有8个字节 。

* 区别五、运行速度

  TCP速度比较慢，而UDP速度比较快，因为TCP必须创建连接，以保证消息的可靠交付和有序性，毕竟TCP协议比UDP复杂。

* 区别六、拥塞机制

  UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）。

* 区别七、流模式（TCP）与数据报模式(UDP);
  TCP面向字节流，实际上是TCP把数据看成一连串无结构的字节流;
  UDP是面向报文的 。
* 区别八、资源占用
  TCP对系统资源要求较多，UDP对系统资源要求较少。
  TCP被认为是重量级的协议，而与之相比，UDP协议则是一个轻量级的协议。因为UDP传输的信息中不承担任何间接创造连接，保证交货或秩序的的信息。这也反映在用于承载元数据的头的大小。
* 区别九、应用
  每一条TCP连接只能是点到点的;UDP支持一对一，一对多，多对一和多对多的交互通信 。基于UDP不需要建立连接，所以且适合多播的环境，UDP是大量使用在游戏和娱乐场所。

![TCPUDP对比.jpg](http://ww1.sinaimg.cn/large/0072fULUgy1gruh212asxj612m0o4tba02.jpg)

总结

1、TCP是面向连接的（在客户端和服务器之间传输数据之前要先建立连接），UDP是无连接的（发送数据之前不需要先建立连接）
2、TCP提供可靠的服务（通过TCP传输的数据。无差错，不丢失，不重复，且按序到达）；UDP提供面向事务的简单的不可靠的传输。
3、UDP具有较好的实时性，工作效率比TCP高，适用于对高速传输和实时性比较高的通讯或广播通信。随着网速的提高，UDP使用越来越多。
4、没一条TCP连接只能是点到点的，UDP支持一对一，一对多和多对多的交互通信。
5、TCP对系统资源要去比较多，UDP对系统资源要求比较少
6、UDP程序结构更加简单
7、TCP是流模式，UDP是数据报模式



基于上面的区别；TCP和UDP的优缺点也很明显了。
UDP 

优点：简单、传输快。
（1）网速的提升给UDP的稳定性提供可靠网络保障，丢包率很低，如果使用应用层重传，能够确保传输的可靠性。
（2）TCP为了实现网络通信的可靠性，使用了复杂的拥塞控制算法，建立了繁琐的握手过程，由于TCP内置的系统协议栈中，极难对其进行改进。采用TCP，一旦发生丢包，TCP会将后续的包缓存起来，等前面的包重传并接收到后再继续发送，延时会越来越大，基于UDP对实时性要求较为严格的情况下，采用自定义重传机制，能够把丢包产生的延迟降到最低，尽量减少网络问题对游戏性造成影响。

缺点：不可靠，不稳定；

UDP应用场景：对实时应用很有用，如IP电话，实时视频会议等
1.面向数据报方式
2.网络数据大多为短消息
3.拥有大量Client
4.对数据安全性无特殊要求
5.网络负担非常重，但对响应速度要求高



TCP：
优点：可靠 稳定
TCP的可靠体现在TCP在传输数据之前，会有三次握手来建立连接，而且在数据传递时，有确认. 窗口. 重传. 拥塞控制机制，在数据传完之后，还会断开来连接用来节约系统资源。

缺点：慢，效率低，占用系统资源高，易被攻击

TCP应用场景：
当对网络质量有要求时，比如HTTP，HTTPS，FTP等传输文件的协议；POP，SMTP等邮件传输的协议。

[面试官：说说UDP和TCP的区别及应用场景](https://zhuanlan.zhihu.com/p/108579426)





**11.TCP 三次握手  四次回收**

三次握手：

第一次握手：建立连接时，客户端发送syn包（syn=j）到服务器，并进入SYN_SENT状态，等待服务器确认；SYN：同步序列编号（Synchronize Sequence Numbers）。

第二次握手：服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包，此时服务器进入SYN_RECV状态；

第三次握手：客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(ack=k+1），此包发送完毕，客户端和服务器进入ESTABLISHED（TCP连接成功）状态，完成三次握手。



四次挥手：

1）第一次挥手：客户端进程发出连接释放报文，并且停止发送数据。释放数据报文首部，FIN=1，其序列号为seq=u（等于前面已经传送过来的数据的最后一个字节的序号加1），此时，客户端进入FIN-WAIT-1（终止等待1）状态。 TCP规定，FIN报文段即使不携带数据，也要消耗一个序号。

2）第二次挥手：服务器收到连接释放报文，发出确认报文，ACK=1，ack=u+1，并且带上自己的序列号seq=v，此时，服务端就进入了CLOSE-WAIT（关闭等待）状态。TCP服务器通知高层的应用进程，客户端向服务器的方向就释放了，这时候处于半关闭状态，即客户端已经没有数据要发送了，但是服务器若发送数据，客户端依然要接受。这个状态还要持续一段时间，也就是整个CLOSE-WAIT状态持续的时间。

3）客户端收到服务器的确认请求后，此时，客户端就进入FIN-WAIT-2（终止等待2）状态，等待服务器发送连接释放报文（在这之前还需要接受服务器发送的最后的数据）。

4）第三次挥手：服务器将最后的数据发送完毕后，就向客户端发送连接释放报文，FIN=1，ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时，服务器就进入了LAST-ACK（最后确认）状态，等待客户端的确认。

5）第四次挥手：客户端收到服务器的连接释放报文后，必须发出确认，ACK=1，ack=w+1，而自己的序列号是seq=u+1，此时，客户端就进入了TIME-WAIT（时间等待）状态。注意此时TCP连接还没有释放，必须经过2MSL（最长报文段寿命）的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。

6）服务器只要收到了客户端发出的确认，立即进入CLOSED状态。同样，撤销TCB后，就结束了这次的TCP连接。可以看到，服务器结束TCP连接的时间要比客户端早一些。



【问题1】为什么连接的时候是三次握手，关闭的时候却是四次挥手？

答：因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，"你发的FIN报文我收到了"。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四步握手。

【问题2】为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？

答：虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是我们必须假象网络是不可靠的，有可以最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。在Client发送出最后的ACK回复，但该ACK可能丢失。Server如果没有收到ACK，将不断重复发送FIN片段。所以Client不能立即关闭，它必须确认Server接收到了该ACK。Client会在发送出ACK之后进入到TIME_WAIT状态。Client会设置一个计时器，等待2MSL的时间。如果在该时间内再次收到FIN，那么Client会重发ACK并再次等待2MSL。所谓的2MSL是两倍的MSL(Maximum Segment Lifetime)。MSL指一个片段在网络中最大的存活时间，2MSL就是一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。

【问题3】为什么不能用两次握手进行连接？

答：3次握手完成两个重要的功能，既要双方做好发送数据的准备工作(双方都知道彼此已准备好)，也要允许双方就初始序列号进行协商，这个序列号在握手过程中被发送和确认。

​       现在把三次握手改成仅需要两次握手，死锁是可能发生的。作为例子，考虑计算机S和C之间的通信，假定C给S发送一个连接请求分组，S收到了这个分组，并发 送了确认应答分组。按照两次握手的协定，S认为连接已经成功地建立了，可以开始发送数据分组。可是，C在S的应答分组在传输中被丢失的情况下，将不知道S 是否已准备好，不知道S建立什么样的序列号，C甚至怀疑S是否收到自己的连接请求分组。在这种情况下，C认为连接还未建立成功，将忽略S发来的任何数据分 组，只等待连接确认应答分组。而S在发出的分组超时后，重复发送同样的分组。这样就形成了死锁。

【问题4】如果已经建立了连接，但是客户端突然出现故障了怎么办？

TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。



**12.TCP粘包的问题**

TCP粘包是指发送方发送的若干包数据到接收方接收时粘成一包，从接收缓冲区看，后一包数据的头紧接着前一包数据的尾。



**什么时候需要考虑粘包问题**

1如果利用tcp每次发送数据，就与对方建立连接，然后双方发送完一段数据后，就关闭连接，这样就不会出现粘包问题（因为只有一种包结构,类似于http协议）。

关闭连接主要是要双方都发送close连接（参考tcp关闭协议）。如：A需要发送一段字符串给B，那么A与B建立连接，然后发送双方都默认好的协议字符如"hello give me sth abour yourself"，然后B收到报文后，就将缓冲区数据接收，然后关闭连接，这样粘包问题不用考虑到，因为大家都知道是发送一段字符。

2如果发送数据无结构，如文件传输，这样发送方只管发送，接收方只管接收存储就ok，也不用考虑粘包3如果双方建立连接，需要在连接后一段时间内发送不同结构数据，如连接后，有好几种结构：

1)"hellogive me sth abour yourself"

2)"Don'tgive me sth abour yourself"

那这样的话，如果发送方连续发送这个两个包出去，接收方一次接收可能会是"hellogive me sth abour yourselfDon't give me sth abour yourself"这样接收方就傻了，到底是要干嘛？不知道，因为协议没有规定这么诡异的字符串，所以要处理把它分包，怎么分也需要双方组织一个比较好的包结构，所以一般可能会**在头加一个数据长度之类的包**，以确保接收。



**粘包出现原因**

简单得说，在流传输中出现，UDP不会出现粘包，因为它有**消息边界**(参考Windows网络编程)

1发送端需要等缓冲区满才发送出去，造成粘包

2接收方不及时接收缓冲区的包，造成多个包接收



为了避免粘包现象，可采取以下几种措施：

（1）对于发送方引起的粘包现象，用户可通过编程设置来避免，TCP提供了强制数据立即传送的操作指令push，TCP软件收到该操作指令后，就立即将本段数据发送出去，而不必等待发送缓冲区满；

（2）对于接收方引起的粘包，则可通过优化程序设计、精简接收进程工作量、提高接收进程优先级等措施，使其及时接收数据，从而尽量避免出现粘包现象；

（3）由接收方控制，将一包数据按结构字段，人为控制分多次接收，然后合并，通过这种手段来避免粘包。

**针对这个问题，一般有3种解决方案：**

**(1)发送固定长度的消息**（发送数据包长度较为稳定(趋于某一固定值)的情况下有较好的效果）

**(2)把消息的尺寸与消息一块发送**（1.报头虽小，但每个包都需要多封装sizeof(_data_head)的数据，积累效应也不可完全忽略。2.接收方的接收动作分成了两次，也就是进行数据读取的操作被增加了一倍，而数据读取操作的recv或者read都是系统调用，这对内核而言的开销是一个不能完全忽略的影响，对程序而言性能影响可忽略（系统调用的速度非常快））

**(3)使用特殊标记来区分消息间隔**（缺陷较为明显：1.接收方需要对数据进行分析，甄别尾部序列。2.尾部序列的确定本身是一个问题）

**方案2较为理想**



**13.TCP如何确保连接的可靠性**

TCP主要提供了检验和、序列号/确认应答、超时重传、最大消息长度、滑动窗口控制等方法实现了可靠性传输。

[TCP的可靠性传输是如何保证的](https://zhuanlan.zhihu.com/p/112317245)



**14.SSL握手过程：**

1. 客户端将它所支持的算法列表和一个用作产生密钥的随机数发送给服务器；

2. 服务器从算法列表中选择一种加密算法，并将它和一份包含服务器公用密钥的证书发送给客户端；该证书还包含了用于认证目的的服务器标识，服务器同时还提供了一个用作产生密钥的随机数；
3. 客户端对服务器的证书进行验证(有关验证证书，可以参考[数字签名](http://www.cnblogs.com/happyhippy/archive/2006/12/23/601357.html))，并抽取服务器的公用密钥；然后，再产生一个称作pre_master_secret的随机密码串，并使用服务器的公用密钥对其进行加密(参考[非对称加/解密](http://www.cnblogs.com/happyhippy/archive/2006/12/23/601357.html))，并将加密后的信息发送给服务器；
4. 客户端与服务器端根据pre_master_secret以及客户端与服务器的随机数值独立计算出加密和MAC密钥(参考[DH密钥交换算法](http://www.cnblogs.com/happyhippy/archive/2006/12/23/601357.html))。
5. 客户端将所有握手消息的MAC值发送给服务器；
6. 服务器将所有握手消息的MAC值发送给客户端。





### MySQL

**1.说下mysql 索引为什么用B+树，不用b树**

* B树所有节点都存储数据，而B+树只有叶子节点存储数据，所以相同的数据B+树的树更加矮壮，这以为这查找磁盘的次数会减少，而且所有数据在叶子节点会使I/O次数和查询性能更加稳定
* B+树叶子节点会使用链表指针连起来，而B树没有，这意味着B树不适合范围查询。



**2.innodb和myinsam引擎的区别。myisam适合用于哪些场景**

* InnoDB支持行锁，而MyISAM不支持
* InnoDB支持事务，而MyISAM不支持
* InnoDB支持外键，而MyISAM不支持
* InnoDB不支持全文索引，而MyISAM支持全文索引



MyISAM适合：(1)做很多count 的计算；(2)插入不频繁，查询非常频繁；(3)没有事务。

InnoDB适合：(1)可靠性要求比较高，或者要求事务；(2)表更新和查询都相当的频繁，并且行锁定的机会比较大的情况。

MyISAM做全库逻辑备份的时候，由于不支持事务，不能使用一致性试图，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。



**3.当前读和快照读的原理**

在MVCC并发控制中，读操作可以分成两类：

快照读，读取的是记录的可见版本 (有可能是历史版本)，不用加锁。

当前读，读取的是记录的最新版本，并且，当前读返回的记录，都会加上锁，保证其他事务不会再并发修改这条记录。







### Redis

**1.redis  rehash过程**

之前我们说过rehash的过程中，需要将ht[0]里面的所有键值对rehash到ht[1]中，但是这个动作并不是集中式的、一次性的；而是分多次，渐进式的。这样做的原因在于假设字典里面的键值对数量非常多，要一次性的rehash的话庞大的计算量可能导致服务器在一段时间内停止服务。下面是渐进式rehash的详细步骤：

- 为ht[1]分配空间，让字典同时拥有ht[0]和ht[1]两个哈希表
- 在字典中维持一个索引计数器变量rehashidx，并将它的值设置为0，表示rehash操作正式开始
- 在rehash进行期间，每次对字典执行添加、删除、修改操作时，程序除了执行指定的操作以外，还会顺带将ht[0]哈希表在rehashidx索引上的所有键值对rehash到ht[1]，当rehash操作完成后，rehashidx的值增加1.
- 随着字典的不断执行，最终在某个时间点上，ht[0]的所有键值对被rehash到ht[1]上，这是程序将rehashidx的值置为-1，表示rehash的操作已完成。



**2.实现查找附近的人，你怎么设计？**

Redis 实现附近的人功能主要通过Geo模块的六个命令。（实际就是使用zset来实现，score是使用geohash将经纬度编码)

- GEOADD：将给定的位置对象(纬度、经度、名字)添加到指定的key;
- GEOPOS：从key里面返回所有给定位置对象的位置(经度和纬度);
- GEODIST：返回两个给定位置之间的距离;
- GEOHASH：返回一个或多个位置对象的Geohash表示;
- GEORADIUS：以给定的经纬度为中心，返回目标集合中与中心的距离不超过给定最大距离的所有位置对象;
- GEORADIUSBYMEMBER：以给定的位置对象为中心，返回与其距离不超过给定最大距离的所有位置对象。

[4种“附近的人”实现方式](https://blog.csdn.net/qq_26545503/article/details/106461821)



**3.如何用redis实现消息的发布订阅**

**Redis的列表类型键可以用来实现队列，并且支持阻塞式读取，可以很容易的实现一个高性能的优先队列。同时在更高层面上，Redis还支持"发布/订阅"的消息模式，可以基于此构建一个聊天系统。**

* 直接使用redis list的数据类型，redis提供了阻塞命令 brpop和blpop。

* 除了实现任务队列外，redis还提供了一组命令可以让开发者实现"发布/订阅"(publish/subscribe)模式

* redis 也可以实现延迟队列，延时队可以通过redis的zset(有序列表)来实现。思路是将消息序列化成一个字符串作为zset的value，这个消息的到期时间作为score 处理 然后用多个线程轮询zset 获取到期的任务进行处理。



**4.单线程的redis为什么这么快？**

* (一)纯内存操作，避免大量访问数据库，减少直接读取磁盘数据，redis将数据储存在内存里面，读写数据的时候都不会受到硬盘 I/O 速度的限制，所以速度快；

  (二)单线程操作，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；

  (三)采用了非阻塞I/O多路复用机制





### 分布式

**1.为什么有rpc调用**

RPC是解决分布式系统通信的一大利器。

RPC的作用：

* 屏蔽远程调用跟本地调用的区别。
* 隐藏底层网络通信的复杂性，让我们更专注于业务逻辑

RPC能够帮助我们解决系统拆分后的通信问题，并且能让我们像调用本地方法一样调用远程方法，利用RPC我们不仅可以很方便的将应用架构从“单体”演进到“微服务化”，而且还能解决实际开发过程中的效率低下，系统耦合等问题，这样可以使得我们的架构更加清晰、健壮、应用可运维度增强。



**2.序列化**

网络传输的数据必须是二进制数据，所以在 RPC 调用中，对入参对象与返回值对象进行序列化与反序列化是一个必须的过程。序列化就是将对象转换成二进制数据的过程，而反序列就是反过来将二进制转换为对象的过程.

实际上任何一种序列化框架，核心思想就是设计一种序列化协议，将对象的类型、属性类型、属性值一一按照固定的格式写到二进制字节流中来完成序列化，再按照固定的格式一一读出对象的类型、属性类型、属性值，通过这些信息重新创建出一个新的对象，来完成反序列化。

序列化优先级从高到低：

安全性>通用性>兼容性> 性能>效率>空间开销



**3.分布式事务2pc的过程**

1.准备阶段：事务管理器给每个参与者发布Prepare消息，每个服务参与者在本地执行事务，并写在Undo/Redo日志，此时事务没有提交。（Undo日志记录修改前数据用于回滚，Redo记录修改后数据用于提交事务后写入数据文件）

2.提交阶段：如果事务管理器收到了参与者的执行失败或者超时消息时，直接给每个参与者进行**回滚**，否则发送**提交**消息。注意：必须在最后阶段释放锁资源



4.分布式雪花算法，百度（uid）怎么解决时钟回拨问题？

通过DefaultUidGenerator的实现可知，它对时钟回拨的处理比较简单粗暴。

直接抛异常?

通过上面对UidGenerator的分析可知，CachedUidGenerator方式主要通过采取如下一些措施和方案规避了时钟回拨问题和增强唯一性：

- **自增列**：UidGenerator的workerId在实例每次重启时初始化，且就是数据库的自增ID，从而完美的实现每个实例获取到的workerId不会有任何冲突。

- **RingBuffer**：UidGenerator不再在每次取ID时都实时计算分布式ID，而是利用RingBuffer数据结构预先生成若干个分布式ID并保存。

- **时间递增**：传统的雪花算法实现都是通过System.currentTimeMillis()来获取时间并与上一次时间进行比较，这样的实现严重依赖服务器的时间。

  而UidGenerator的时间类型是AtomicLong，且通过incrementAndGet()方法获取下一次的时间，从而脱离了对服务器时间的依赖，也就不会有时钟回拨的问题

  （这种做法也有一个**小问题**，即分布式ID中的时间信息可能并不是这个ID真正产生的时间点，例如：获取的某分布式ID的值为3200169789968523265，它的反解析结果为{"timestamp":"2019-05-02 23:26:39","workerId":"21","sequence":"1"}，但是这个ID可能并不是在"2019-05-02 23:26:39"这个时间产生的）。







### Spring

**1.Spring中 bean的生命周期**

如上图所示，Bean 的生命周期还是比较复杂的，下面来对上图每一个步骤做文字描述:

1. Spring启动，查找并加载需要被Spring管理的bean，进行Bean的实例化
2. Bean实例化后对将Bean的引入和值注入到Bean的属性中
3. 如果Bean实现了BeanNameAware接口的话，Spring将Bean的Id传递给setBeanName()方法
4. 如果Bean实现了BeanFactoryAware接口的话，Spring将调用setBeanFactory()方法，将BeanFactory容器实例传入
5. 如果Bean实现了ApplicationContextAware接口的话，Spring将调用Bean的setApplicationContext()方法，将bean所在应用上下文引用传入进来。
6. 如果Bean实现了BeanPostProcessor接口，Spring就将调用他们的postProcessBeforeInitialization()方法。
7. 如果Bean 实现了InitializingBean接口，Spring将调用他们的afterPropertiesSet()方法。类似的，如果bean使用init-method声明了初始化方法，该方法也会被调用
8. 如果Bean 实现了BeanPostProcessor接口，Spring就将调用他们的postProcessAfterInitialization()方法。
9. 此时，Bean已经准备就绪，可以被应用程序使用了。他们将一直驻留在应用上下文中，直到应用上下文被销毁。
10. 如果bean实现了DisposableBean接口，Spring将调用它的destory()接口方法，同样，如果bean使用了destory-method 声明销毁方法，该方法也会被调用。

```java

/**
 * @author xxm
 * @date 2021/6/22 10:40
 */
public class TestBeanLife implements BeanNameAware, BeanFactoryAware, ApplicationContextAware, InitializingBean , DisposableBean {
    @Override
    public void setBeanName(String name) {
        System.out.println("BeanNameAware-setBeanName");
    }

    @Override
    public void setBeanFactory(BeanFactory beanFactory) throws BeansException {
        System.out.println("BeanFactoryAware-setBeanFactory");
    }

    @Override
    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
        System.out.println("ApplicationContextAware-setApplicationContext");
    }

    public void init(){
        System.out.println("TestBeanLife init-method");
    }

    @Override
    public void afterPropertiesSet() throws Exception {
        System.out.println("InitializingBean-afterPropertiesSet");
    }

    @Override
    public void destroy() throws Exception {
        System.out.println("DisposableBean-destroy");
    }
    
    public void destroyMethod(){
        System.out.println("destroy-method");
    }
}

public class MyBeanPostProcessor implements BeanPostProcessor {
    @Override
    public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException {
        System.out.println("BeanPostProcessor-postProcessBeforeInitialization-{" + beanName + "}");
        return bean;
    }

    @Override
    public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException {
        System.out.println("BeanPostProcessor-postProcessAfterInitialization-{" + beanName + "}");
        return bean;
    }
}

/*
<bean id="testBeanLife" class="com.javadoop.springaoplearning.beanLife.TestBeanLife" init-method="init" destroy-method="destroyMethod"/>
    <bean class="com.javadoop.springaoplearning.beanLife.MyBeanPostProcessor" />

输出：
BeanNameAware-setBeanName
BeanFactoryAware-setBeanFactory
ApplicationContextAware-setApplicationContext
BeanPostProcessor-postProcessBeforeInitialization-{testBeanLife}
InitializingBean-afterPropertiesSet
TestBeanLife init-method
BeanPostProcessor-postProcessAfterInitialization-{testBeanLife}
DisposableBean-destroy
destroy-method
*/

```



**2.Spring的加载过程**(refresh过程？)

* Application加载xml
* AbstractApplicationContext的refresh函数载入Bean定义过程
* AbstractApplicationContext子类的refreshBeanFactory()方法
* AbstractRefreshableApplicationContext子类的loadBeanDefinitions方法
* AbstractBeanDefinitionReader读取Bean定义资源
* 资源加载器获取要读入的资源
* XmlBeanDefinitionReader加载Bean定义资源
* DocumentLoader将Bean定义资源转换为Document对象
* XmlBeanDefinitionReader解析载入的Bean定义资源文件
* DefaultBeanDefinitionDocumentReader对Bean定义的Document对象解析
* BeanDefinitionParserDelegate解析Bean定义资源文件中的元素
* BeanDefinitionParserDelegate解析元素
* 解析元素的子元素
* 解析子元素
* 解析过后的BeanDefinition在IoC容器中的注册（IoC容器的初始化结束）
* DefaultListableBeanFactory向IoC容器注册解析后的BeanDefinition（依赖注入开始）
* AbstractBeanFactory通过getBean向IoC容器获取被管理的Bean
* AbstractAutowireCapableBeanFactory创建Bean实例对象
* createBeanInstance方法创建Bean的java实例对象
* SimpleInstantiationStrategy类使用默认的无参构造方法创建Bean实例化对象
* populateBean方法对Bean属性的依赖注入
* BeanDefinitionValueResolver解析属性值
* BeanWrapperImpl对Bean属性的依赖注入
  



[一位大神的博客-Spring IOC 容器源码分析](https://javadoop.com/post/spring-ioc)

[详解Spring IOC加载全过程](https://blog.csdn.net/qq_34203492/article/details/83865450)



**3.AOP的使用**

目前 Spring AOP 一共有三种配置方式

- Spring 1.2 **基于接口的配置**：最早的 Spring AOP 是完全基于几个接口的，想看源码的同学可以从这里起步。
- Spring 2.0 **schema-based 配置**：Spring 2.0 以后使用 XML 的方式来配置，使用 命名空间 `<aop />`
- Spring 2.0 **@AspectJ 配置**：使用注解的方式来配置，这种方式感觉是最方便的，还有，这里虽然叫做 `@AspectJ`，但是这个和 AspectJ 其实没啥关系。



[Spring AOP 使用介绍，从前世到今生](https://javadoop.com/post/spring-aop-intro)

[Spring AOP 源码解析](https://javadoop.com/post/spring-aop-source)



**4.Spring框架中都用到了哪些设计模式？**

**1. 代理模式**：在AOP和remoting中被用的比较多。

**2. 单例模式**：在spring配置文件中定义的bean默认为单例模式。

**3. 模板方法模式**：用来解决代码重复的问题。

**4. 前端控制器模式**：Spring提供了DispatcherServlet来对请求进行分发。

**5. 依赖注入模式**：贯穿于BeanFactory / ApplicationContext接口的核心理念。

**6. 工厂模式**：BeanFactory用来创建对象的实例。



**5.spring中Bean的作用域**

**1. singleton**：Spring IoC容器中只会存在一个共享的Bean实例，无论有多少个Bean引用它，始终指向同一对象。Singleton作用域是Spring中的缺省作用域。

**2. prototype**：每次通过Spring容器获取prototype定义的bean时，容器都将创建一个新的Bean实例，每个Bean实例都有自己的属性和状态，而singleton全局只有一个对象。

**3. request**：在一次Http请求中，容器会返回该Bean的同一实例。而对不同的Http请求则会产生新的Bean，而且该bean仅在当前Http Request内有效。

**4. session**：在一次Http Session中，容器会返回该Bean的同一实例。而对不同的Session请求则会创建新的实例，该bean实例仅在当前Session内有效。

**5. global Session**：在一个全局的Http Session中，容器会返回该Bean的同一个实例，仅在使用portlet context时有效。



**6.springmvc的核心是什么，请求的流程是怎么处理的，控制反转怎么实现的**

**核心**：控制反转和面向切面

**请求处理流程**：

1. 首先用户发送请求到前端控制器，前端控制器根据请求信息（如URL）来决定选择哪一个页面控制器进行处理并把请求委托给它，即以前的控制器的控制逻辑部分；
2. 页面控制器接收到请求后，进行功能处理，首先需要收集和绑定请求参数到一个对象，并进行验证，然后将命令对象委托给业务对象进行处理；处理完毕后返回一个ModelAndView（模型数据和逻辑视图名）；
3. 前端控制器收回控制权，然后根据返回的逻辑视图名，选择相应的视图进行渲染，并把模型数据传入以便视图渲染；
4. 前端控制器再次收回控制权，将响应返回给用户。

**控制反转如何实现**：

- 我们每次使用spring框架都要配置xml文件，这个xml配置了bean的id和class。
- spring中默认的bean为单实例模式，通过bean的class引用反射机制可以创建这个实例。
- 因此，spring框架通过反射替我们创建好了实例并且替我们维护他们。
- A需要引用B类，spring框架就会通过xml把B实例的引用传给了A的成员变量。